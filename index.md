<p align="center">
<img src="assets/teaser.png" />
<h1>Abstract</h1>
 </p>
 
<p align="justify">
High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of <em>conditional exploration</em> of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating  conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes. For example, for faces we vary camera pose, illumination variation, expression, skin tone, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other parallel works.
</p>
<p align="center">
<img src="assets/1.gif" height="300" /><img src="assets/2.gif" height="300" /></p>

<p align="center">
<img src="assets/car1.gif" height="300" /><img src="assets/car2.gif" height="300" /><img src="assets/car3.gif" height="300" /><img src="assets/car4.gif" height="300" /></p>
<p align="center">
<h1>Overview Video</h1>
 </p>

<div class="container">
  <div class="title">Overview Video</div>
  <div style="text-align: center;">
    <iframe width="960" height="540" src="https://www.youtube.com/embed/0Vbj9xFgoUw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>

<p align="center">
<h1>Method</h1>
 </p>
 
 <p align="center">
<img src="assets/styleFlow_pipeline.png" /></p>


<p align="justify">
Attribute-conditioned editing using StyleFlow. Starting from a source image, we support attribute-conditioned editing by using a reverse inference followed by a forward inference though a sequence of CNF blocks. Here, z denotes the variable of the prior distribution and w denotes the intermediate weight vector of the StyleGAN.
</p>

<p align="center">
<h1>Sampling</h1>
 </p>
 
 <p align="center">
<img src="assets/sampling.png" /></p>


<p align="justify">
Attribute-conditioned sampling using StyleFlow. Here we show sampling results for attribute specifications of \textit{females with glasses in a target pose}~(top); \textit{50-year old males with facial hair}~(middle); and \textit{smiling 5-year old children in a target pose}~(bottom).
</p>

<p align="center">
<h1>Attribute Transfer</h1>
 </p>
 
 <p align="center">
<img src="assets/sota.png" /></p>


<p align="justify">
Attribute-conditioned edits where source images are edited using multiple attributes from the target images. Upper set uses pose, expression, and illumination from the respective target images; bottom set uses eyeglass, facial hair, age, and gender from the respective target images.
</p>


<p align="center">
<h1>Comparison to Existing Methods</h1>
 </p>
 
 <p align="center">
<img src="assets/comapre.png" /></p>


<p align="justify">
Comparison of StyleFlow with other contemporary systems Image2StyleGAN and InterfaceGAN.
</p>

<div class="container">
  <div class="bibtex">Bibtex</div>
<pre>
  @misc{zhu2019sean,
    title={SEAN: Image Synthesis with Semantic Region-Adaptive Normalization},
    author={Peihao Zhu and Rameen Abdal and Yipeng Qin and Peter Wonka},
    year={2019},
    eprint={1911.12861},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</pre>
</div>

<div class="container">
  <div class="acknowledgement">Acknowledgement</div>
  <div class="body" style="font-size: 11pt;;">
    We thank Wamiq Reyaz Para for helpful comments. This  work  was  supported  by  the KAUST Office of Sponsored Research (OSR) under AwardNo. OSR-CRG2018-3730.
  </div>

</div>


<div class="container">
  <div class="ref">Related Work</div>
  <div class="citation">
    <img src="./assets/SPADE.png">
    <a href="https://nvlabs.github.io/SPADE/">
      Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-YanZhu.
      Semantic image synthesis with spatially-adaptive nor-malization.
      CVPR, 2019.
    </a>
  </div>
  <div class="citation">
    <img src="./assets/styleGAN.png">
    <a href="https://github.com/NVlabs/stylegan">
      Tero Karras, Samuli Laine, Timo Aila.
      A Style-Based Generator Architecture for Generative Adversarial Networks
      CVPR, 2019.
    </a>
  </div>
  <div class="citation">
    <img src="./assets/Pix2PixHD.png">
    <a href="https://tcwang0509.github.io/pix2pixHD/">
      Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.
      High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.
      CVPR, 2018.
    </a>
  </div>








